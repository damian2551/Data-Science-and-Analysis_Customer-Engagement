---
title: "PDS Assessment 2_22049939"
author: "Damian Nguyen"
date: "2025-05-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Assumptions**\
- Any rows having NA or blank strings in user_id or review_id will be removed as they are not meaningful for further calculations.\
- Other variables which are not used in the analysis but having NA or blank strings may not need to be removed.\
- In reviews dataset, users are assumed to be in the same State with the business they reviewed.Â 

**Packages installation**

```{r setup}
library(tidyverse)
library(knitr)
library(ggplot2)
library(dplyr)
library(kableExtra)
```

# Question 1

## 1.1) Data Wrangling

For this question, users dataset will be used. The first step is to review the dataset, and then format data if required.

```{r}
users <- read.csv("users.csv") #import data

#Review data
head(users)
str(users)

colSums(is.na(users)) #count if there are any NA values in each column
colSums(users == "") #count if there are any blank strings ("") in each column
```

**Findings:** Despite there is no NA values from the users dataset, the following columns - user_id, name, member_since have the blank strings ("").\
\

Reviewing top 15 users by review_count:

```{r}
top15_ReviewCount <- users %>%  arrange(desc(review_count)) %>% select(name, review_count) %>% head(15)
top15_ReviewCount
```

**Conclusion:** Since top 15 users have the same number of review count (99) which is not meaningful for intepretation afterward. users dataset will be merged with reviews dataset for better analaysis.\

Before joining, reviews data would be examined for usability:

```{r}
reviews <- read.csv("reviews.csv") #import reviews dataset

#Examine data
str(reviews)
colSums(is.na(reviews)) #No NA Values
colSums((reviews==""))
```

```{r}
#check duplicated data 
colSums(sapply(reviews, duplicated))
```

**Conclusion**\
- There is no NA values from the reviews dataset. However, there are empty string values in review_id user_id.\
- Despite having duplicated in other values, the review_id which is essential to identify a particular information about a review is still unique. Therefore, other duplicates are acceptable.\
- Only user_id variable should be addressed if there are any duplicates for further analysis.\

Remove any rows having empty strings values in user_id and review_id from reviews for further analysis:

```{r}
cleaned_reviews <- reviews %>% filter(review_id != "") %>% filter(user_id != "")
colSums((cleaned_reviews==""))
```

**Joint data:** reviews will left joint with `users` since a user can review multiple times. Therefore, this approach will ensure not missing any review_id, which will be used for counting the number of review later per user later on.

```{r}
reviewsUsers <- cleaned_reviews %>% left_join(users, by = c("user_id" = "user_id"))
head(reviewsUsers)
```

## 1.2) Three User Groups:

After checking, the variable member_since should be formatted to Date variable in order to categorise into three groups later on

```{r}
reviewsUsers$member_since <- as.Date(reviewsUsers$member_since) #change to Date variable.
head(reviewsUsers$member_since) #double check the reformatted member_since
```

This step is to create 3 different user groups - Veteran, Intermediate and New based on their joining date, using member_since Note: When filtering, blank strings are automatically transferred to NA values, and will be removed using drop_na().

```{r}
Veteran <- reviewsUsers %>%
  filter(reviewsUsers$member_since < as.Date('2017-01-01')) %>% 
  drop_na(member_since) #removes any rows where the member_since column is NA (missing) 

Intermediate <- reviewsUsers %>%
  filter(between(reviewsUsers$member_since, as.Date('2017-01-01'), as.Date('2022-12-31'))) %>% drop_na(member_since)

New <- reviewsUsers %>%
  filter(reviewsUsers$member_since > as.Date('2022-12-31')) %>% drop_na(member_since)

#Count if there are NA values in user_id columns
sum(is.na(Veteran$user_id))
sum(is.na(Intermediate$user_id))
sum(is.na(New$user_id))
```

Conclusion:\
-There are no NA values in three datasets - Veteran, Intermediate and New.\
-The three datasets are ready for further analysis.\

## 1.3) Calculate the numbers of users, their average review stars and average number of reviews per user.

Calculate the number of unique users

```{r}
#numbers of unique users, using count distinct as there are duplicates in each User Group
numVeteran <- Veteran %>% summarise(count = n_distinct(user_id))
numIntermediate <- Intermediate %>% summarise(count = n_distinct(user_id))
numNew<- New %>% summarise(count = n_distinct(user_id))

#convert to numberic for tabulating
numVeteran <- as.numeric(numVeteran)
numIntermediate <- as.numeric(numIntermediate)
numNew <- as.numeric(numNew)
```

Calculate the average review rating

```{r}
#average review
avg_Veteran <- Veteran %>% filter(!is.na(stars), stars != "") %>%# Remove NA and blank strings
  mutate(stars = as.numeric(stars)) %>% # Convert to numeric
  summarise(avg_star = mean(stars, na.rm = TRUE))

avg_Intermediate <- Intermediate %>% filter(!is.na(stars), stars != "") %>%# Remove NA and blank strings 
  mutate(stars = as.numeric(stars)) %>% # Convert to numeric
  summarise(avg_star = mean(stars, na.rm = TRUE))

avg_New <- New %>% filter(!is.na(stars), stars != "") %>%# Remove NA and blank strings 
  mutate(stars = as.numeric(stars)) %>% # Convert to numeric
  summarise(avg_star = mean(stars, na.rm = TRUE))

## convert to numeric for tabulation
avg_Veteran <- as.numeric(avg_Veteran)
avg_Intermediate <- as.numeric(avg_Intermediate)
avg_New <- as.numeric(avg_New)
```

Since the goal is to calculate the average number of reviews **per user**, unique number of users will be used instead of the number of user as a whole.

```{r}
#average review count - unique r
avgReCount_Veteran <- length(Veteran$review_id) / numVeteran
avgReCount_Intermediate <- length(Intermediate$review_id) / numIntermediate
avgReCount_New <- length(New$review_id) / numNew
```

Tabulate the data using kable:

```{r}
# Create a summary data frame
summaryTable <- data.frame(row.names = Group <- c("Veteran", "Intermediate", "New"),
  Number_of_Unique_Users = c(numVeteran, numIntermediate, numNew),
  Average_Review_Stars = c(avg_Veteran, avg_Intermediate, avg_New),
  Average_Review_Count = c(avgReCount_Veteran, avgReCount_Intermediate,avgReCount_New))

colnames(summaryTable) <- c("Unique Users", "Average Stars", "Average Review Count") #rename headers

# Display with table using kable()
kable(summaryTable, caption = "User Summary by Groups", digits = 3) %>% #round to 3 decimals
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE,
                position = "center")
```

**Findings**\
- Intermediate has a highest number of members (`r numIntermediate`), while Veteran has the lowest number.\
- There are less significant differences between their average review length, indicating similar user behaviours across three groups.\
- However, as old users (Veteran), their average review should be higher compared to other groups, while their figure is the lowest, indicating quite low user behaviour from this group. - Given the considerable number of members, Intermediate average rating is the second-highest, implying a good engagement from this group.\
- The average rating from Veteran (old customers) is the lowest, along with low average review length, implying their low engagement with the community.\
- The average review star of New users is the highest, along with their second-highest position in number of unique users, indicating a good engagement from them and good attraction from the community recently.\

## 1.4) Visualisation of Average Review Stars by User Groups.

```{r}
# Add a column member_type to the reviewsUsers dataset for data visualisation
users2 <- reviewsUsers %>% mutate( member_type = case_when(member_since < as.Date("2017-01-01") ~ "Veteran", between(member_since, as.Date("2017-01-01"), as.Date("2022-12-31")) ~ "Intermediate", member_since > as.Date("2022-12-31") ~ "New",
        TRUE ~ NA_character_ # Handles NA values
)) %>% drop_na(member_type) #remove NA values if required

head(users2) #check if the data is correct
# users2 is ready for visualisation
```

Visualisation of the Average Rating by User Groups. Boxplot is used since it can demonstrate the distribution and mean of each group.

```{r}
ggplot(users2, aes(x = member_type, y = stars, fill = member_type)) + geom_boxplot(outlier.shape = NA, alpha = 0.7) + labs(title = "Comparison of Average Rating by User Groups", x = "User Group", y = "Star Rating") + scale_fill_brewer(palette = "Set2") +theme_minimal(base_size = 14)
```

**Findings**\
- There is no statistically significant difference between the means of average rating, and the distribution (IQR) across the group.\
- Most of the ratings mostly fall around 3 which is similar to the table above.\

Since there is less difference between the average rating, the distribution of rating will be further examined. Therefore, bar chart will be applied in this case to visualise the distribution of 3 user groups.

```{r}
#barplot visualising the count of rating stars by user groups
ggplot(users2, mapping = aes(x = stars, fill = factor(member_type))) + geom_bar(position = "dodge", color = "black") +
labs(title = "Distribution of Star Ratings by Member Type", x = "Star Rating",y = "Count") + theme_minimal() #note average_stars must be continuous variables to do histogram
```

**Findings:** From rating 1 to 5, the distribution across 3 user groups is relatively similar. This could potentially caused the least difference of their average rating between user groups.

## 1.5) Conclusions:

Three user groups have low differences between their average rating and average length of review.\
Intermediate group has the dominant number of users with second position in average rating, indicating a relatively good engagement and behaviour.\
New users with shorter time of joining but have the equivalent average length of reviews. They could be potential for future expansion of the community.\

# Question 2:

## 2.1) Data Wrangling for businessPGA

```{r}
PGA <- read.csv("businessesPGA.csv") #import data

#examine data
str(PGA)
head(PGA)
colSums(is.na(PGA)) #no NA values
colSums((PGA=="")) #check if there are blank strings
```

**Findings:**\
-There is no NA values from the dataset PGA. However, the important variables for further analysis have blank strings, they are state, business_id.\
- Some variables should be changed to factor variable: state, categories, business_group.\

This step is to format the data:

```{r}
length(unique(PGA$business_id)) #to check whether all value in business_id is unique

#format data, 
PGA$state <- as.factor(PGA$state)
PGA$categories <- as.factor(PGA$categories)
PGA$business_group <- as.factor(PGA$business_group)

#recheck if there are any NA values after formatting
colSums(is.na(PGA))

#removing any rows that have blank strings values for state
cleaned_PGA <- PGA %>% filter(!is.na(state), state != "")
colSums(is.na(cleaned_PGA))
```

**Findings:** There is no NA values and blank strings after formatting. The dataset `cleaned_PGA` is ready to use

## 2.2) The average reviews star by State (PGA)

```{r}
#Calculate average raring by states
avgTable_byState <- aggregate(cleaned_PGA$stars, list(cleaned_PGA$state), FUN = mean) 

#Add column names for clarity in visualisation
colnames(avgTable_byState) <- c("state", "mean_stars")
avgTable_byState <- avgTable_byState %>% drop_na(c(state,mean_stars)) %>% #Remove NA values in both columns
  filter(!is.na(state), state != "")

#Visualisation
ggplot(avgTable_byState, aes(x = reorder(state, mean_stars), y = mean_stars, fill = state)) + geom_bar(stat = "identity")

#count numbers of state
length(unique(PGA$state)) #52 (50 states, 1 DC, 1 Blank row calculated from blank string values)
```

**Findings:** As there are 50 different States, the plot lacks of clarity and interpretation from data. For appropriate interpretability, only top 10 States having highest average rating will be visualised for further analysis.

```{r}
#Select top 10
top10 <- avgTable_byState %>% arrange(desc(mean_stars)) %>% slice_head(n = 10)

#Visualisation of top 10
ggplot(top10, aes(x = reorder(state, mean_stars), y = mean_stars, fill = state)) + geom_bar(stat = "identity") + 
  geom_text(aes(label = round(mean_stars, 3)), size = 3.5) + #adding numbers for better reading
  coord_flip() +
  labs(title = "Top 10 States by Average Review Stars", x = "State", y = "Average Stars") + theme_minimal() + theme(legend.position = "none")
```

**Findings**\
- The average review stars are slightly different accross the states.\
- Top 3 states with highest average rating are Texas, New York, and Milano.\

## 2.3) The number of reviews and the number of unique users (PGA)

To count unique number of users by state, joining database is required. Two datasets will be used joinining - reviews and `PGA`. reviews will left join with `PGA` dataset as we need to calculate the number of reviews later on.\
If we right join, some review data will be lost.\

```{r}
cleaned_PGA <- cleaned_PGA %>% drop_na(business_id) #remove any rows having missing business_id

#joining reviews and PGA
merge_PGA <- cleaned_reviews %>% left_join(cleaned_PGA, by = "business_id") %>% 
  drop_na(c(user_id,state)) #remove any rows having missing user_id or states

#recheck NA values or blank strings after joining
colSums(is.na(merge_PGA))
colSums((merge_PGA==""))
```

**Findings:**\
- There are still blank strings values in business_id.\
- Therefore, any rows with blank strings in business_id will be removed.\

```{r}
cleaned_merge_PGA <- merge_PGA %>% filter(!is.na(business_id), business_id != "")
colSums((cleaned_merge_PGA==""))
```

After joining, duplicates are more likely to appear. This step is to check if there are any duplicates and whether they are acceptable

```{r}
#check duplicated data 
colSums(sapply(cleaned_merge_PGA, duplicated))
```

**Findings:**\
- There is no NA values from the joint dataset - `cleaned_merge_PGA`.\
- Despite having duplicated in other values, the review_id which is essential to identify a particular information about a review is still unique. Therefore, other duplicates are acceptable.\
- Only user_id variable should be addressed if there are any duplicates for further analysis.\

Count the number of unique users by States

```{r}
uniqueUserCount <- cleaned_merge_PGA %>% distinct(user_id, state) %>% count(state, name = "unique_users") 
head(uniqueUserCount)
```

**Assumption:** A user can be in more than 1 States, as long as their user id is unique in that particular State.\

Count the number of reviews by States

```{r}
#count based on numbers of review_id
numReviews <- aggregate(review_id ~ state, data = cleaned_merge_PGA, FUN = length)

colnames(numReviews) <- c("state", "review_count")

# Convert state to factor variable
numReviews$state <- as.factor(numReviews$state)
numReviews <- numReviews %>% filter(!is.na(state), state != "") #Remove rows having blank strings in state
head(numReviews)
```

Summary Table of Average Star, Count of Review, and Count of Unique Users by States **Note**: Count of Review and Count of Unique Users will be calculated based on the top 10 States having highest average rating.

```{r}
joined_data <- top10 %>% left_join(numReviews, by = "state") %>% left_join(uniqueUserCount, by = "state")

colnames(joined_data) <- c("State", "Average Stars", "Review Count", "Unique Users")

#using kable to tabulate the top 10 States
kable(joined_data, caption = "Summary of 10 States (PGA)", digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE, position = "center")

```

**Findings**\
- Texas (TX) with the highest average rating also has the relatively high number of number of unique users and reviews, indicating a positive and active engagement from users in this state.\
- Ohio (OH) has the highest number number of unique users and reviews and ranked in 4 out of 10 among top rating States. The company could take advantages of this high number of users for future expansion.\
- Indiana (IN) has a lowest rating among top 10, and quite low number of unique users and review count, indicating low potential. Thus, the company should not prioritise IN over other States in top 10.\

## 2.4) Visualisation of Unique users by State (PGA)

As there are 51 different States, only top 10 States having the highest number of unique users will be used.

```{r}
#Select top 10
top10_Unqiue <- uniqueUserCount %>% arrange(desc(unique_users)) %>% drop_na(state) %>% slice_head(n = 10) 

#visualisation
ggplot(top10_Unqiue, aes(x = reorder(state, unique_users), y = unique_users, fill = state)) + geom_bar(stat = "identity") + 
  geom_text(aes(label = unique_users,  size = 3)) +
  coord_flip() +
  labs(title = "Top 10 States by Unique Users", x = "State", y = "Number of Unique Users") + theme_minimal() + theme(legend.position = "none")
```

**Findings:**\
- As ranked in the third for number of unique users, along with its relatively high rank in average rating, Ohio could be the potential State with high number of user and high engagement. The company should consider OH for future targeting.\

(Find the common State in top 10 by average rating and unique users)

```{r}
common_10 <- top10_Unqiue %>% inner_join(top10, by = "state")
common_10
```

-   HI and MO also have the highest number of users from the community, indicating a potential base for expansion as well.\
